{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd500df",
   "metadata": {},
   "source": [
    "The following code uses a 2D Convolutional Neural Network (CNN) to detect whether a page is being flipped or not based on a single image. The model consists of two convolutional layers, each using three 3×3 filters, followed by max pooling layers to reduce spatial dimensions and retain the most important features.\n",
    "\n",
    "During training, the input images are processed in batches of 32 images, resulting in 75 batches per epoch. For each batch, the model performs forward propagation, computes the loss, and updates the filter and dense layer weights using gradient descent. This process is repeated across 10 epochs, with the learned weights from each epoch carried forward to the next.\n",
    "After each epoch, the model is evaluated on a separate testing dataset to compute validation accuracy and validation loss. These testing images are not used for learning and do not affect the model’s weights. After training is complete, the final learned weights are used to generate predictions on the full test dataset.\n",
    "\n",
    "The model achieves a Test F1 score of 98.21%, indicating strong performance. The F1 score is an appropriate evaluation metric for this problem because it balances precision and recall, accounting for both false positives (incorrectly detecting a page flip) and false negatives (missing an actual page flip), which are both important in the context of page flip detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381828b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2392 images belonging to 2 classes.\n",
      "Found 597 images belonging to 2 classes.\n",
      "Class indices: {'flip': 0, 'notflip': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarvagyasharma/tf_env_tf/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-12-09 13:20:04.527322: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-12-09 13:20:04.527492: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-12-09 13:20:04.527503: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-12-09 13:20:04.527700: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-12-09 13:20:04.527712: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2700</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">86,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │            \u001b[38;5;34m84\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │            \u001b[38;5;34m84\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2700\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m86,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">86,633</span> (338.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m86,633\u001b[0m (338.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">86,633</span> (338.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m86,633\u001b[0m (338.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 13:20:05.597728: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 248ms/step - accuracy: 0.5895 - loss: 0.6586 - val_accuracy: 0.6466 - val_loss: 0.6148\n",
      "Epoch 2/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 239ms/step - accuracy: 0.7768 - loss: 0.4828 - val_accuracy: 0.8727 - val_loss: 0.3748\n",
      "Epoch 3/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 239ms/step - accuracy: 0.9130 - loss: 0.2568 - val_accuracy: 0.9363 - val_loss: 0.2060\n",
      "Epoch 4/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 238ms/step - accuracy: 0.9490 - loss: 0.1712 - val_accuracy: 0.8911 - val_loss: 0.2590\n",
      "Epoch 5/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 238ms/step - accuracy: 0.9678 - loss: 0.1090 - val_accuracy: 0.9598 - val_loss: 0.1308\n",
      "Epoch 6/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 238ms/step - accuracy: 0.9833 - loss: 0.0635 - val_accuracy: 0.9782 - val_loss: 0.0852\n",
      "Epoch 7/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 237ms/step - accuracy: 0.9900 - loss: 0.0451 - val_accuracy: 0.9749 - val_loss: 0.0837\n",
      "Epoch 8/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 241ms/step - accuracy: 0.9875 - loss: 0.0434 - val_accuracy: 0.9682 - val_loss: 0.0957\n",
      "Epoch 9/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 237ms/step - accuracy: 0.9921 - loss: 0.0299 - val_accuracy: 0.9832 - val_loss: 0.0639\n",
      "Epoch 10/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 238ms/step - accuracy: 0.9921 - loss: 0.0301 - val_accuracy: 0.9816 - val_loss: 0.0591\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 176ms/step\n",
      "Test F1 score: 0.9821\n"
     ]
    }
   ],
   "source": [
    "# All imports required for the model\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def clean_path(path_input):\n",
    "    \"\"\"\n",
    "    Remove unwanted quotes from copied macOS folder paths.\n",
    "    Example input:  '/Users/...'\n",
    "    Example output: /Users/...\n",
    "    \"\"\"\n",
    "    return path_input.replace(\"'\", \"\").replace('\"', \"\").strip()\n",
    "\n",
    "\n",
    "def create_generators(train_dir, test_dir, img_size, batch_size):\n",
    "    \"\"\"\n",
    "    Create training and testing image generators for loading data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize pixel values from [0,255] to [0,1]\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "    # Generator for training images\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        directory=train_dir,      # Folder containing flip and notflip\n",
    "        target_size=img_size,     # Resize images to desired size\n",
    "        batch_size=batch_size,    # Number of images per batch\n",
    "        class_mode=\"binary\",      # 0 or 1 output labels\n",
    "        shuffle=True              # Shuffle training images\n",
    "    )\n",
    "\n",
    "    # Generator for testing images\n",
    "    test_gen = test_datagen.flow_from_directory(\n",
    "        directory=test_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"binary\",\n",
    "        shuffle=False             # Keep order for F1 scoring\n",
    "    )\n",
    "\n",
    "    return train_gen, test_gen\n",
    "\n",
    "\n",
    "def build_cnn_model(img_height, img_width):\n",
    "    \"\"\"\n",
    "    Build a simple 2 layer CNN with 3 filters each.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sequential model builds layers in order\n",
    "    model = models.Sequential([\n",
    "\n",
    "        # First convolution layer\n",
    "        layers.Conv2D(\n",
    "            filters=3,                  # Number of filters\n",
    "            kernel_size=(3, 3),         # Filter size\n",
    "            activation=\"relu\",          # Activation function\n",
    "            input_shape=(img_height,    # Input image height\n",
    "                         img_width,     # Input image width\n",
    "                         3)             # RGB channels\n",
    "        ),\n",
    "\n",
    "        # First max pooling layer to reduce spatial size\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # Second convolution layer\n",
    "        layers.Conv2D(\n",
    "            filters=3,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=\"relu\"\n",
    "        ),\n",
    "\n",
    "        # Second pooling layer\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # Flatten the output for dense layers\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Dense hidden layer\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "\n",
    "        # Output layer with sigmoid for binary classification\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    # Compile the model with optimizer, loss, metrics\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_f1(model, test_gen):\n",
    "    \"\"\"\n",
    "    Compute F1 score on the test set predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reset generator to start from first image\n",
    "    test_gen.reset()\n",
    "\n",
    "    # Predict probabilities for each test image\n",
    "    y_prob = model.predict(test_gen)\n",
    "\n",
    "    # Convert probabilities to 0 or 1 predictions\n",
    "    y_pred = (y_prob > 0.5).astype(\"int32\").ravel()\n",
    "\n",
    "    # True labels from the generator\n",
    "    y_true = test_gen.classes\n",
    "\n",
    "    # Calculate F1 score\n",
    "    return f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# MAIN EXECUTION CELL\n",
    "# ===============================\n",
    "\n",
    "# Image size for resizing images\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "img_size = (img_height, img_width)\n",
    "\n",
    "# Number of images per training batch\n",
    "batch_size = 32\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "# Paste your folders here (with or without quotes)\n",
    "raw_train_dir = input(\"Paste training folder path: \")\n",
    "raw_test_dir = input(\"Paste testing folder path: \")\n",
    "\n",
    "# Clean the folder paths to remove quotes\n",
    "train_dir = clean_path(raw_train_dir)\n",
    "test_dir = clean_path(raw_test_dir)\n",
    "\n",
    "# Create data generators for loading images\n",
    "train_gen, test_gen = create_generators(\n",
    "    train_dir, test_dir, img_size, batch_size\n",
    ")\n",
    "\n",
    "# Display class mapping (flip or notflip)\n",
    "print(\"Class indices:\", train_gen.class_indices)\n",
    "\n",
    "# Build the CNN model\n",
    "model = build_cnn_model(img_height, img_width)\n",
    "\n",
    "# Print the model architecture\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_gen\n",
    ")\n",
    "\n",
    "# Compute F1 score\n",
    "f1 = evaluate_f1(model, test_gen)\n",
    "print(f\"Test F1 score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env_tf",
   "language": "python",
   "name": "tf_env_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
